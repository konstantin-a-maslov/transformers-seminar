{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758c2146",
   "metadata": {},
   "source": [
    "# Transformers seminar\n",
    "\n",
    "This notebook is a supplementary material for the seminar on vision transformers (UT-ITC, 31 Oct 2022). It mainly discusses a Google's paper about applying transformers from natural language processing to computer vision problems with minimal modifications.\n",
    "\n",
    "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\n",
    "https://doi.org/10.48550/arxiv.2010.11929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f4029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:46:24.448613: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab000ead",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "\n",
    "We will play with simple MNIST dataset here as the purpose of this notebook is just to demonstrate how ViT works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979b9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# extract dataset and image sizes\n",
    "dataset_size, image_height, image_width = x_train.shape\n",
    "\n",
    "# extract the number of classes\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# reshape the images so they have the channel dimension\n",
    "x_train = x_train[:, :, :, np.newaxis]\n",
    "x_test = x_test[:, :, :, np.newaxis]\n",
    "\n",
    "# normalise the inputs\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# one-hot encoding of the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# divide `x_train` and `y_train` into train and val subsets\n",
    "val_ratio = 0.2\n",
    "val_samples = int(val_ratio * dataset_size)\n",
    "x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n",
    "x_train, y_train = x_train[val_samples:], y_train[val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d435c2d",
   "metadata": {},
   "source": [
    "Let's choose one random sample and use it later to show how it is transformed at different stages of ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67e2b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_idx = np.random.choice(len(x_train))\n",
    "sample = x_train[sample_idx]\n",
    "sample_label = y_train[sample_idx]\n",
    "\n",
    "plt.title(np.argmax(sample_label))\n",
    "plt.imshow(sample, vmin=0.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a17352",
   "metadata": {},
   "source": [
    "#### Patch extraction and embedding\n",
    "\n",
    "First, we implement a layer for patch extraction. To do it there is a nice `tensorflow` function called `extract_patches`.\n",
    "\n",
    "<img src=\"figures/vit_patch_embedding.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c04cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtraction(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(PatchExtraction, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # make patches\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=inputs,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"SAME\"\n",
    "        )\n",
    "        # flatten tensor from two-dimensional grid of patches to one-dimensional\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        patches_count, depth = tf.shape(patches)[1] * tf.shape(patches)[2], tf.shape(patches)[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, patches_count, depth])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee52df",
   "metadata": {},
   "source": [
    "Now we can make a one-image batch from the sample image and apply this layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29945790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:46:26.862043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:26.887126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:26.887784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:26.888741: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 16:46:26.889154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:26.889760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:26.890337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:27.547158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:27.547781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:27.548339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 16:46:27.548856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7713 MB memory:  -> device: 0, name: NVIDIA RTXA6000-24C, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 28, 28, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make batch\n",
    "batch = tf.convert_to_tensor(sample)\n",
    "batch = tf.expand_dims(batch, axis=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3fd6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 49, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a layer and apply it to the batch\n",
    "patch_size = 4\n",
    "patches = PatchExtraction(patch_size)(batch)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1da87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAHVCAYAAADRta6NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAALbElEQVR4nO3dv4tldx2A4XtnJmzipFCyxixCWGNYJaks7IIIMpBKAzpYiJ2CRFgMYrNgmS7pQv4GYVJEQRAGQUNAYiOCqMlKgr9IQAuLKK67mWM3UTByzztz7rk38zz1+XA/fLlc3vttznIYhmEBAMAoO3MvAACwjUQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAMHeqg8e7BxOucfaHZ8czb3CKWc7HWc7HWc7HWc7HWc7rYt2vm6iAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAECyHYRjmXgIAYNu4iQIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABHurPniwczjlHmt3fHI09wqnnO10nO10ytnuPnJt9Mxj3/vl6Jkbl18dPbPzwM3RM1N54uUnR8/8/TN/mWCT87Ht39tNtklnu1hcvPN1EwUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIFj5BcQAZ/XXT983euY79/169MztYfTI4tL4kck8/eD3R898/UtPjZ7Zf+GV0TPAu9xEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIvIAYYMM8fNf41yH/80Pj/xPvj54A/pObKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAYDkMwzD3EgAA28ZNFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAg2Fv1wYOdwyn3WLvjk6O5VzjlbKfjbKdTzvbqz+8ZPfPcR18ePVNcuvL6Wj5nFU/94sujZ377hSujZ+786c+jZ4pt/95usk0628Xi4p2vmygAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgWPndeQBn9ZXLP5t7ha1w8+37R8+s6z14wLvcRAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABHtzLwBsp70HPjJ65u7l7fBJF++/3h//9sHRM/cv3jr/RYD/6+L9OgEAnAMRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAATLYRiGuZcAANg2bqIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAMHeqg8e7BxOucfaHZ8czb3CKWc7HWc7navPPTN65tUvPj/BJufj0pXX517h1OOP3hg9885vbk6wyfnYpO+t34RpXbTzdRMFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACBY+QXEwPna/cTDc69wJj954tkwdc+57/G/XPvhN0bP/P5rEywSndx8Y+4VgBW4iQIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEHgBMcxld7v/w1zZXc/LhIudf+zOvcKZDHfuzL0CsILt/hUHAJiJiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAADBchiGYe4lAAC2jZsoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAECwt+qDBzuHU+6xdscnR3OvcMrZTmeTz3b3kWujZ370q6cn2KS59eZDc6/wnj559M3RM29c//YEmzSb/L0t/CZMZ5POdrG4eOfrJgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIVn53HnC+3rn30twrAHAGbqIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAReQAwz2X/mrblXAOAM3EQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAATLYRiGuZcAANg2bqIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAMHeqg8e7BxOucfaHZ8czb3CKWc7nU0+2/2XPjx65sXHnp9gk+bWmw+t5XNeu/2v0TPf+uqTo2d+/NMbo2emssnf28JvwnQ26WwXi4t3vm6iAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEK7+AGHhvy089Onrmc5dfmmCT95+7liejZ+7s+2kDpucmCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABA4C2dcA7e/vi9o2c++4HXJtjk/edje3ePnvnD47sTbALw39xEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIvIAYzsH+C6+Mnvnu9c+PnnnxwdEjW+93t2+Nnrn6g9vjP+j6+BHgYnMTBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgWA7DMMy9BADAtnETBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAI9lZ98GDncMo91u745GjuFU452+k42+k42+k42+k422ldtPN1EwUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBARAEABCIKACAQUQAAgYgCAAhEFABAIKIAAAIRBQAQiCgAgEBEAQAEIgoAIBBRAACBiAIACEQUAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQCCiAAACEQUAEIgoAIBgOQzDMPcSAADbxk0UAEAgogAAAhEFABCIKACAQEQBAAQiCgAgEFEAAIGIAgAIRBQAQPBv7mT+DzZCmhQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert to numpy\n",
    "patches_np = patches[0].numpy()\n",
    "\n",
    "# reshape (as pixels are flattened within patches)\n",
    "patches_np = np.reshape(patches_np, (-1, patch_size, patch_size))\n",
    "\n",
    "# and visualize\n",
    "for patch_idx, patch in enumerate(patches_np):\n",
    "    plt.subplot(image_height // patch_size, image_width // patch_size, patch_idx + 1)\n",
    "    plt.imshow(patch, vmin=0.0, vmax=1.0)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc433279",
   "metadata": {},
   "source": [
    "Now we can implement patch embedding, which is a simple linear projection of the patches. Here, we decided to implement it as a separate layer that wraps a `Dense` layer to make the model's final code more readable and give you a more straightforward setup for modifications. However, in this situation, it was completely unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f420ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.projection = tf.keras.layers.Dense(embedding_size)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        embedded_patches = self.projection(inputs)\n",
    "        return embedded_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd2435",
   "metadata": {},
   "source": [
    "Let's proceed further and embed our patches obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b54258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:46:29.652858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 49, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "embedded_patches = PatchEmbedding(embedding_size)(patches)\n",
    "embedded_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1382a49",
   "metadata": {},
   "source": [
    "Pay attention to how the tensor shapes changed so far:\n",
    "1. Input image [28, 28, 1]\n",
    "2. 4x4 patches [49, 16] = [(28/4)\\*(28/4), 4*4]\n",
    "3. Embedding (linear projection) [49, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185231d5",
   "metadata": {},
   "source": [
    "#### Class token\n",
    "\n",
    "Now we will add an 'artificial' **learnable** token to the embedding that will represent the output class. It has the same size as an embedded patch and is processed later in the same way as other patches.\n",
    "\n",
    "<img src=\"figures/vit_class_token.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55305e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AddClassToken, self).__init__()\n",
    "        \n",
    "    # note the use of the `build` method\n",
    "    # it is useful to define weights/sublayers that are dependent on the input shape\n",
    "    def build(self, input_shape):\n",
    "        self.class_token = self.add_weight(\n",
    "            name=\"class_token\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        patches_with_class_token = tf.map_fn(\n",
    "            lambda x: tf.concat((x, [self.class_token]), axis=0),\n",
    "            inputs\n",
    "        )\n",
    "        return patches_with_class_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e26e5",
   "metadata": {},
   "source": [
    "As usual, let's apply this layer to the sample patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5bbba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/eouser/anaconda3/envs/tfgeo/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_with_class_token = AddClassToken()(embedded_patches)\n",
    "patches_with_class_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d9360",
   "metadata": {},
   "source": [
    "We can see now that there is a new 'patch' (50 vs 49), we can have a closer look at what is inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b57ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([-0.0481041 ,  0.01329737, -0.04044154, -0.08065665, -0.00512561,\n",
       "        0.00590261,  0.10439717, -0.07927667, -0.01027794, -0.06902959,\n",
       "       -0.00357601, -0.00471488,  0.07645728,  0.03128856, -0.04204969,\n",
       "       -0.0956307 ,  0.01259496, -0.01205202,  0.04817585, -0.03920084,\n",
       "       -0.01595327, -0.05591167,  0.04546424,  0.04373219,  0.01404479,\n",
       "       -0.02501643, -0.04159253, -0.05617034,  0.033245  , -0.03469346,\n",
       "       -0.05828989, -0.04405783], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_with_class_token[0, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97d365",
   "metadata": {},
   "source": [
    "As expected, something random. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62051bee",
   "metadata": {},
   "source": [
    "#### Positional embedding\n",
    "\n",
    "Transformers heavily rely on self-attention, which is a global operator and, at the same time, do not differentiate how spatially its inputs are related. Therefore, we have to implement something that encodes the location of the patches. In ViT, the authors propose to add **learnable** vectors to the embedded patches (and hope that they manage to learn the spatial dependencies, in the paper they managed).\n",
    "\n",
    "<img src=\"figures/vit_positional_encoding.png\" width=480 />\n",
    "\n",
    "Let's implement exactly the same idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f2b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AddPositionalEmbedding, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            name=\"positional_embedding\",\n",
    "            shape=input_shape[1:],\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.positional_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f9ec4",
   "metadata": {},
   "source": [
    "And we can check again if the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ecc606a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positionally_embedded_patches = AddPositionalEmbedding()(patches_with_class_token)\n",
    "positionally_embedded_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b6bcc",
   "metadata": {},
   "source": [
    "**However**, this naive approach is not appropriate when you are working with images of different sizes (and, thus, having different numbers of patches per image). One of the workarounds, in this case, is to interpolate positional embeddings, which is not implemented here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2143c",
   "metadata": {},
   "source": [
    "#### Transformer block\n",
    "\n",
    "Following the paper, we can easily implement the transformer block described there. Happily, `tf.keras` has all the required operations, we just need to combine them in the right order.\n",
    "\n",
    "<img src=\"figures/vit_encoder.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63a6eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, mlp_size, n_heads, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.mhsa = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=embedding_size // n_heads\n",
    "        )\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = tf.keras.layers.Dense(mlp_size, activation=tf.nn.gelu)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(embedding_size, activation=tf.nn.gelu)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # MHSA branch\n",
    "        x1 = self.norm1(inputs)\n",
    "        x1 = self.mhsa(x1, x1)\n",
    "        # residual connection 1\n",
    "        x2 = self.add([inputs, x1])\n",
    "        # MLP branch\n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.dense1(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        x3 = self.dense2(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        # residual connection 2\n",
    "        outputs = self.add([x2, x3])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26b8a0",
   "metadata": {},
   "source": [
    "Run it to check on obvious bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffb334ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:46:30.062343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_block_output = TransformerBlock(32, 64, 4, 0.1)(positionally_embedded_patches)\n",
    "transformer_block_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d578373",
   "metadata": {},
   "source": [
    "#### Extract class representation\n",
    "\n",
    "In ViT, the class of the whole image is represented by one token introduced in the very beginning (of course, there are alternatives, in the paper they consider global average pooling as well). We need to extract this token to classify further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19e2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ExtractClassToken, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs[:, -1, :] # all images, last patch, all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794d313",
   "metadata": {},
   "source": [
    "Traditionally, we are checking how our sample image is transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "435353bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_token = ExtractClassToken()(transformer_block_output)\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53274822",
   "metadata": {},
   "source": [
    "#### Classification head\n",
    "\n",
    "As we are considering a classification problem, we would like to add a classification head to the model. A regular MLP is used in ViT, which is very straightforward.\n",
    "\n",
    "<img src=\"figures/vit_classifier.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e373833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, mlp_size, n_classes, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(mlp_size, activation=tf.keras.activations.tanh)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(n_classes, activation=tf.keras.activations.softmax)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden = self.dense1(inputs)\n",
    "        hidden = self.dropout(hidden)\n",
    "        outputs = self.dense2(hidden)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb674fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.13443133, 0.11477651, 0.12003591, 0.11622641, 0.09524829,\n",
       "        0.09020421, 0.08977901, 0.1246384 , 0.06969218, 0.04496778]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified = MLP(32, 10, 0.1)(class_token)\n",
    "classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f6aa1",
   "metadata": {},
   "source": [
    "#### Assembling everything together\n",
    "\n",
    "Fine, everything is prepared to build a ViT model.\n",
    "\n",
    "<img src=\"figures/vit.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c70f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViT(\n",
    "    input_shape, n_classes, patch_size=16, embedding_size=768, mlp_size=3072,\n",
    "    n_blocks=12, n_heads=12, dropout=0.1, name=\"ViT\", **kwargs\n",
    "):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    \n",
    "    patches = PatchExtraction(patch_size)(inputs)\n",
    "    patches = PatchEmbedding(embedding_size)(patches)\n",
    "    patches = AddClassToken()(patches)\n",
    "    patches = AddPositionalEmbedding()(patches)\n",
    "    for _ in range(n_blocks):\n",
    "        patches = TransformerBlock(embedding_size, mlp_size, n_heads, dropout)(patches)\n",
    "    class_token = ExtractClassToken()(patches)\n",
    "    outputs = MLP(mlp_size, n_classes, dropout)(class_token)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name=name, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb8e4c",
   "metadata": {},
   "source": [
    "Now we can create a model and do some final checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edb51516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ViT\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 512, 512, 1)]     0         \n",
      "                                                                 \n",
      " patch_extraction_1 (PatchEx  (None, 1024, 256)        0         \n",
      " traction)                                                       \n",
      "                                                                 \n",
      " patch_embedding_1 (PatchEmb  (None, 1024, 768)        197376    \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " add_class_token_1 (AddClass  (None, 1025, 768)        768       \n",
      " Token)                                                          \n",
      "                                                                 \n",
      " add_positional_embedding_1   (None, 1025, 768)        787200    \n",
      " (AddPositionalEmbedding)                                        \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_3 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_4 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_5 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_6 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_8 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_9 (Transf  (None, 1025, 768)        7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_10 (Trans  (None, 1025, 768)        7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_11 (Trans  (None, 1025, 768)        7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_12 (Trans  (None, 1025, 768)        7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " extract_class_token_1 (Extr  (None, 768)              0         \n",
      " actClassToken)                                                  \n",
      "                                                                 \n",
      " mlp_1 (MLP)                 (None, 1000)              5435368   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91,475,176\n",
      "Trainable params: 91,475,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_b16 = ViT(input_shape=(512, 512, 1), n_classes=1000)\n",
    "vit_b16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047cfcd0",
   "metadata": {},
   "source": [
    "This model is ViT-B/16 as they call it in the paper. We can now compare the number of parameters that we got with the number of parameters reported by the authors. If they match, we are on the right track. So, excluding the classification head and working with 512x512 images, we have got $91\\,475\\,176 - 5\\,435\\,368 = 86\\,039\\,808$ parameters. \n",
    "\n",
    "<img src=\"figures/vit_params_table.png\" width=384 />\n",
    "\n",
    "The authors report 86M parameters, however, it is not very clear if it is for 512x512 images and whether they exclude the classification head. Anyway, the number is really close, therefore, we expect that our implementation is also correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63a195ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vit_b16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db87742",
   "metadata": {},
   "source": [
    "#### Training the model and evaluating it\n",
    "\n",
    "First, let's define another variant of ViT that is not too complicated for MNIST, we call it `vit_toy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62dae505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ViT-Toy\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " patch_extraction_2 (PatchEx  (None, 49, 16)           0         \n",
      " traction)                                                       \n",
      "                                                                 \n",
      " patch_embedding_2 (PatchEmb  (None, 49, 64)           1088      \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " add_class_token_2 (AddClass  (None, 50, 64)           64        \n",
      " Token)                                                          \n",
      "                                                                 \n",
      " add_positional_embedding_2   (None, 50, 64)           3200      \n",
      " (AddPositionalEmbedding)                                        \n",
      "                                                                 \n",
      " transformer_block_13 (Trans  (None, 50, 64)           33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_14 (Trans  (None, 50, 64)           33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_15 (Trans  (None, 50, 64)           33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_16 (Trans  (None, 50, 64)           33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_17 (Trans  (None, 50, 64)           33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_18 (Trans  (None, 50, 64)           33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " extract_class_token_2 (Extr  (None, 64)               0         \n",
      " actClassToken)                                                  \n",
      "                                                                 \n",
      " mlp_2 (MLP)                 (None, 10)                9610      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 214,794\n",
      "Trainable params: 214,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_toy = ViT(\n",
    "    input_shape=(image_height, image_width, 1),\n",
    "    n_classes=n_classes,\n",
    "    patch_size=4,\n",
    "    embedding_size=64,\n",
    "    mlp_size=128,\n",
    "    n_blocks=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    name=\"ViT-Toy\"\n",
    ")\n",
    "\n",
    "vit_toy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5ac71",
   "metadata": {},
   "source": [
    "Compile it and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1a0f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_toy.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    # note that the use of AdamW is a more common practice when training transformers\n",
    "    # however, we could not get good results with it for vit_toy and MNIST\n",
    "    # optimizer=tfa.optimizers.AdamW(learning_rate=1e-5, weight_decay=1e-4),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa342def",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"vit_toy.h5\", monitor=\"val_loss\", save_best_only=True, save_weights_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\"vit_toy.csv\"),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=21, verbose=1, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.1, patience=10, verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ceb04991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:46:42.045526: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5620e136c120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-18 16:46:42.045557: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA RTXA6000-24C, Compute Capability 8.6\n",
      "2023-04-18 16:46:42.259363: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-18 16:46:43.721264: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 75s 610ms/step - loss: 1.3646 - categorical_accuracy: 0.5082 - val_loss: 0.6078 - val_categorical_accuracy: 0.7937 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "94/94 [==============================] - 56s 599ms/step - loss: 0.4862 - categorical_accuracy: 0.8386 - val_loss: 0.3665 - val_categorical_accuracy: 0.8836 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "94/94 [==============================] - 56s 594ms/step - loss: 0.3209 - categorical_accuracy: 0.8982 - val_loss: 0.2965 - val_categorical_accuracy: 0.9062 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "94/94 [==============================] - 56s 601ms/step - loss: 0.2537 - categorical_accuracy: 0.9203 - val_loss: 0.2033 - val_categorical_accuracy: 0.9370 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "94/94 [==============================] - 57s 602ms/step - loss: 0.2158 - categorical_accuracy: 0.9320 - val_loss: 0.1730 - val_categorical_accuracy: 0.9463 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "94/94 [==============================] - 56s 597ms/step - loss: 0.1790 - categorical_accuracy: 0.9435 - val_loss: 0.1528 - val_categorical_accuracy: 0.9533 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "94/94 [==============================] - 56s 598ms/step - loss: 0.1528 - categorical_accuracy: 0.9523 - val_loss: 0.1441 - val_categorical_accuracy: 0.9560 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "94/94 [==============================] - 57s 602ms/step - loss: 0.1353 - categorical_accuracy: 0.9574 - val_loss: 0.1337 - val_categorical_accuracy: 0.9585 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "94/94 [==============================] - 57s 604ms/step - loss: 0.1228 - categorical_accuracy: 0.9612 - val_loss: 0.1227 - val_categorical_accuracy: 0.9621 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "94/94 [==============================] - 56s 600ms/step - loss: 0.1075 - categorical_accuracy: 0.9668 - val_loss: 0.1348 - val_categorical_accuracy: 0.9588 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "94/94 [==============================] - 57s 607ms/step - loss: 0.0944 - categorical_accuracy: 0.9704 - val_loss: 0.1140 - val_categorical_accuracy: 0.9648 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "94/94 [==============================] - 55s 588ms/step - loss: 0.0918 - categorical_accuracy: 0.9712 - val_loss: 0.0968 - val_categorical_accuracy: 0.9699 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "94/94 [==============================] - 56s 597ms/step - loss: 0.0830 - categorical_accuracy: 0.9730 - val_loss: 0.1046 - val_categorical_accuracy: 0.9687 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "94/94 [==============================] - 56s 595ms/step - loss: 0.0741 - categorical_accuracy: 0.9765 - val_loss: 0.0996 - val_categorical_accuracy: 0.9711 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "94/94 [==============================] - 56s 599ms/step - loss: 0.0704 - categorical_accuracy: 0.9776 - val_loss: 0.0879 - val_categorical_accuracy: 0.9737 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "94/94 [==============================] - 56s 597ms/step - loss: 0.0605 - categorical_accuracy: 0.9805 - val_loss: 0.0802 - val_categorical_accuracy: 0.9772 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "94/94 [==============================] - 56s 596ms/step - loss: 0.0579 - categorical_accuracy: 0.9816 - val_loss: 0.0920 - val_categorical_accuracy: 0.9728 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "94/94 [==============================] - 56s 601ms/step - loss: 0.0510 - categorical_accuracy: 0.9837 - val_loss: 0.0973 - val_categorical_accuracy: 0.9736 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "94/94 [==============================] - 56s 595ms/step - loss: 0.0518 - categorical_accuracy: 0.9829 - val_loss: 0.0846 - val_categorical_accuracy: 0.9759 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "94/94 [==============================] - 56s 601ms/step - loss: 0.0481 - categorical_accuracy: 0.9840 - val_loss: 0.0865 - val_categorical_accuracy: 0.9747 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "94/94 [==============================] - 55s 589ms/step - loss: 0.0460 - categorical_accuracy: 0.9847 - val_loss: 0.0813 - val_categorical_accuracy: 0.9757 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "94/94 [==============================] - 57s 605ms/step - loss: 0.0391 - categorical_accuracy: 0.9869 - val_loss: 0.1282 - val_categorical_accuracy: 0.9641 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "94/94 [==============================] - 56s 598ms/step - loss: 0.0417 - categorical_accuracy: 0.9868 - val_loss: 0.0886 - val_categorical_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "94/94 [==============================] - 56s 593ms/step - loss: 0.0336 - categorical_accuracy: 0.9891 - val_loss: 0.0985 - val_categorical_accuracy: 0.9745 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "94/94 [==============================] - 55s 591ms/step - loss: 0.0358 - categorical_accuracy: 0.9879 - val_loss: 0.0932 - val_categorical_accuracy: 0.9755 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0376 - categorical_accuracy: 0.9877\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "94/94 [==============================] - 56s 598ms/step - loss: 0.0376 - categorical_accuracy: 0.9877 - val_loss: 0.0844 - val_categorical_accuracy: 0.9777 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "94/94 [==============================] - 56s 597ms/step - loss: 0.0173 - categorical_accuracy: 0.9943 - val_loss: 0.0696 - val_categorical_accuracy: 0.9803 - lr: 1.0000e-04\n",
      "Epoch 28/2000\n",
      "94/94 [==============================] - 56s 592ms/step - loss: 0.0119 - categorical_accuracy: 0.9966 - val_loss: 0.0702 - val_categorical_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 29/2000\n",
      "94/94 [==============================] - 57s 604ms/step - loss: 0.0107 - categorical_accuracy: 0.9967 - val_loss: 0.0687 - val_categorical_accuracy: 0.9816 - lr: 1.0000e-04\n",
      "Epoch 30/2000\n",
      "94/94 [==============================] - 56s 595ms/step - loss: 0.0098 - categorical_accuracy: 0.9971 - val_loss: 0.0683 - val_categorical_accuracy: 0.9814 - lr: 1.0000e-04\n",
      "Epoch 31/2000\n",
      "94/94 [==============================] - 56s 599ms/step - loss: 0.0081 - categorical_accuracy: 0.9979 - val_loss: 0.0718 - val_categorical_accuracy: 0.9816 - lr: 1.0000e-04\n",
      "Epoch 32/2000\n",
      "94/94 [==============================] - 56s 595ms/step - loss: 0.0085 - categorical_accuracy: 0.9976 - val_loss: 0.0690 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-04\n",
      "Epoch 33/2000\n",
      "94/94 [==============================] - 56s 601ms/step - loss: 0.0081 - categorical_accuracy: 0.9977 - val_loss: 0.0701 - val_categorical_accuracy: 0.9817 - lr: 1.0000e-04\n",
      "Epoch 34/2000\n",
      "94/94 [==============================] - 57s 603ms/step - loss: 0.0073 - categorical_accuracy: 0.9982 - val_loss: 0.0715 - val_categorical_accuracy: 0.9819 - lr: 1.0000e-04\n",
      "Epoch 35/2000\n",
      "94/94 [==============================] - 56s 597ms/step - loss: 0.0076 - categorical_accuracy: 0.9980 - val_loss: 0.0739 - val_categorical_accuracy: 0.9818 - lr: 1.0000e-04\n",
      "Epoch 36/2000\n",
      "94/94 [==============================] - 56s 595ms/step - loss: 0.0068 - categorical_accuracy: 0.9980 - val_loss: 0.0715 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-04\n",
      "Epoch 37/2000\n",
      "94/94 [==============================] - 56s 598ms/step - loss: 0.0059 - categorical_accuracy: 0.9985 - val_loss: 0.0723 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-04\n",
      "Epoch 38/2000\n",
      "94/94 [==============================] - 57s 606ms/step - loss: 0.0063 - categorical_accuracy: 0.9981 - val_loss: 0.0729 - val_categorical_accuracy: 0.9816 - lr: 1.0000e-04\n",
      "Epoch 39/2000\n",
      "94/94 [==============================] - 57s 603ms/step - loss: 0.0059 - categorical_accuracy: 0.9983 - val_loss: 0.0740 - val_categorical_accuracy: 0.9821 - lr: 1.0000e-04\n",
      "Epoch 40/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0058 - categorical_accuracy: 0.9984\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "94/94 [==============================] - 56s 593ms/step - loss: 0.0058 - categorical_accuracy: 0.9984 - val_loss: 0.0769 - val_categorical_accuracy: 0.9813 - lr: 1.0000e-04\n",
      "Epoch 41/2000\n",
      "94/94 [==============================] - 56s 595ms/step - loss: 0.0047 - categorical_accuracy: 0.9990 - val_loss: 0.0753 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 42/2000\n",
      "94/94 [==============================] - 56s 594ms/step - loss: 0.0046 - categorical_accuracy: 0.9989 - val_loss: 0.0749 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 43/2000\n",
      "94/94 [==============================] - 55s 591ms/step - loss: 0.0043 - categorical_accuracy: 0.9988 - val_loss: 0.0745 - val_categorical_accuracy: 0.9821 - lr: 1.0000e-05\n",
      "Epoch 44/2000\n",
      "94/94 [==============================] - 56s 596ms/step - loss: 0.0049 - categorical_accuracy: 0.9987 - val_loss: 0.0758 - val_categorical_accuracy: 0.9823 - lr: 1.0000e-05\n",
      "Epoch 45/2000\n",
      "94/94 [==============================] - 57s 604ms/step - loss: 0.0052 - categorical_accuracy: 0.9984 - val_loss: 0.0758 - val_categorical_accuracy: 0.9820 - lr: 1.0000e-05\n",
      "Epoch 46/2000\n",
      "94/94 [==============================] - 56s 598ms/step - loss: 0.0044 - categorical_accuracy: 0.9991 - val_loss: 0.0755 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 47/2000\n",
      "94/94 [==============================] - 56s 598ms/step - loss: 0.0047 - categorical_accuracy: 0.9986 - val_loss: 0.0747 - val_categorical_accuracy: 0.9823 - lr: 1.0000e-05\n",
      "Epoch 48/2000\n",
      "94/94 [==============================] - 56s 600ms/step - loss: 0.0039 - categorical_accuracy: 0.9992 - val_loss: 0.0751 - val_categorical_accuracy: 0.9820 - lr: 1.0000e-05\n",
      "Epoch 49/2000\n",
      "94/94 [==============================] - 57s 606ms/step - loss: 0.0043 - categorical_accuracy: 0.9991 - val_loss: 0.0749 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 50/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0042 - categorical_accuracy: 0.9988\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "94/94 [==============================] - 56s 596ms/step - loss: 0.0042 - categorical_accuracy: 0.9988 - val_loss: 0.0747 - val_categorical_accuracy: 0.9823 - lr: 1.0000e-05\n",
      "Epoch 51/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0045 - categorical_accuracy: 0.9989Restoring model weights from the end of the best epoch: 30.\n",
      "94/94 [==============================] - 56s 600ms/step - loss: 0.0045 - categorical_accuracy: 0.9989 - val_loss: 0.0749 - val_categorical_accuracy: 0.9821 - lr: 1.0000e-06\n",
      "Epoch 51: early stopping\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 2000\n",
    "batch_size = 512\n",
    "\n",
    "history = vit_toy.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=training_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dd25e",
   "metadata": {},
   "source": [
    "And evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccaa173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 30ms/step - loss: 0.0624 - categorical_accuracy: 0.9843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06241050362586975, 0.9843000173568726]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_toy.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73ca80",
   "metadata": {},
   "source": [
    "Not the state-of-the-art, of course, but still impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652d699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
