{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758c2146",
   "metadata": {},
   "source": [
    "# Transformers seminar\n",
    "\n",
    "This notebook is a supplementary material for the seminar on visual transformers (UT-ITC, 29 Oct 2022). It mainly discusses a Google paper about applying transformers from natural language processing to computer vision problems with minimal modifications.\n",
    "\n",
    "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\n",
    "https://doi.org/10.48550/arxiv.2010.11929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f4029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab000ead",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "\n",
    "We will play with simple MNIST dataset here as the purpose of this notebook is just to demonstrate how ViT works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979b9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# extract dataset and image sizes\n",
    "dataset_size, image_height, image_width = x_train.shape\n",
    "\n",
    "# extract the number of classes\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# reshape the images so they have the channel dimension\n",
    "x_train = x_train[:, :, :, np.newaxis]\n",
    "x_test = x_test[:, :, :, np.newaxis]\n",
    "\n",
    "# normalise the inputs\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# one-hot encoding of the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# divide `x_train` and `y_train` into train and val subsets\n",
    "val_ratio = 0.2\n",
    "val_samples = int(val_ratio * dataset_size)\n",
    "x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n",
    "x_train, y_train = x_train[val_samples:], y_train[val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d435c2d",
   "metadata": {},
   "source": [
    "Let's choose one random sample and use it later to show how it is transformed at different stages of ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67e2b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_idx = np.random.choice(len(x_train))\n",
    "sample = x_train[sample_idx]\n",
    "sample_label = y_train[sample_idx]\n",
    "\n",
    "plt.title(np.argmax(sample_label))\n",
    "plt.imshow(sample, vmin=0.0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a17352",
   "metadata": {},
   "source": [
    "#### Patch extraction and embedding\n",
    "\n",
    "First, we implement a layer for patch extraction. To do it there is a nice `tensorflow` function called `extract_patches`.\n",
    "\n",
    "<img src=\"figures/vit_patch_embedding.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c04cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtraction(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(PatchExtraction, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # make patches\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=inputs,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"SAME\"\n",
    "        )\n",
    "        # flatten tensor from two-dimensional grid of patches to one-dimensional\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        patches_count, depth = tf.shape(patches)[1] * tf.shape(patches)[2], tf.shape(patches)[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, patches_count, depth])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee52df",
   "metadata": {},
   "source": [
    "Now we can make a one-image batch from the sample image and apply this layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29945790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 28, 28, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make batch\n",
    "batch = tf.convert_to_tensor(sample)\n",
    "batch = tf.expand_dims(batch, axis=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3fd6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 49, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a layer and apply it to the batch\n",
    "patch_size = 4\n",
    "patches = PatchExtraction(patch_size)(batch)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d1da87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAHVCAYAAADRta6NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOCklEQVR4nO3da2yeZR3H8bYriYOpAUWYjHUcJhtnRQzIGyTWoAgqpKAxHqIhKEzCIQIecEBQE4makC2ROBIjMRALgiQSYlXA6CbBKaicN4goEmA4jhvi2tt3hcyQ9P9b7+d52n4+r/tLL65U/HK9ufubpmn6AAAoGej2AQAAZiIRBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQGJzqDw4PjLR5jo4bmxjt9hEmudv2uNv2uNv2uNv2uNt2zbX79RIFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAgf6maZpuHwIAYKbxEgUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAACBwan+4PDASJvn6LixidFuH2GSu22Pu21Pp+72qHv+W95cuvu95c3Ang+XN20Zuvo75c07Tr+rhZNMj7n4d9spvXS3fX1z7369RAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBgyh8gBnitZz91dHlz7tevK2+O2/mf5c3+t59V3jzy8fKkNaved015s/qtx5Q345ueKW+AV3mJAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgIAPEEOXPPfJo7p9hB1y3eVXlDeLB3cub45dcV55s9+Nd5Y3fT30AeLVx3+wvBnf9Gh5M3DY8vJm4p77yxuYrbxEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQMAHiGEaDC7aq7z52EW/Dn7T+cGmHbvNm1fevPN7K8qbhTetK29muvEN9Y8JJx47YdfyZuVoZ87WloHDDyxvJu6+r4WTMBt4iQIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAIBAf9M0TbcPAQAw03iJAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgMDgVH9weGCkzXN03NjEaLePMMndtie52/7BKf/PYtKDq95V3mw48QflzU4LN5Y3bfnA7eeUN81xj0//QabJTP+7nbd8aXmz9cpXyptfHXhjedNTf7dHXlLeNOvvnf6DTJNe+rvt65t7/3/mJQoAICCiAAACIgoAICCiAAACIgoAICCiAAACIgoAICCiAAACIgoAICCiAAACIgoAIFD/SBjMck+c9Z7yZsOJq8qbHz63d3lz5sLypDUDp2wpb8ZbOMdsNG/pvuXNSTesLW9Of/M/ypuvPlX/TuQVPfR323//o+VN08I5mB28RAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEDAB4hhO1sWduZzo1de+5Hy5sxLWzhIaHzz5m4f4XWNH1v/SG4vOeXm+seEP7TLhvLmoN9/sbzZ57Mby5u+F+uTtkxsqX84G16PlygAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAI+AAxbGe3+zrzez5z6liwOnfaz5EaOHRZeTPxlwfKm3nLl5Y356+5przp6/tKsGnHyQseKW/ef/mXy5uhq9aVNxPlBcxeXqIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAg0N80TdPtQwAAzDReogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAwONUfHB4YafMcHTc2MdrtI0xyt+1J7nZgl13Kmxdu2KO8ueOQ68ubnRZuLG/a8szje5U3pz50Wnnz+UW/K29GFjxT3vTS3R5wyffLm8WXrW3hJNNjpv87oZf10t329c29+/USBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAIEpf4AY5oqJl14qbxZ8bX55c+TKT5Q3d3+4PGnNnf/Ztby5ddnPWzjJ/3t+4uXy5i0tnCO12wMT3T4CMAVeogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACDgA8QwDZr195Y3u58U/KIe+i7txd/8XHkzfPnq8mbjtq3lzaJ5O5U3veSj3xgrb277zZLyZnzTM+UN8CovUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABAQUQAAAREFABDwAWLoksE99+j2EXbIoWf8tSO/55wj6l9qfuRLB5Q3D11cnrTmvF0fLm/WrD6mvNnv7HnlzfiTT5U3MFt5iQIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAIBAf9M0TbcPAQAw03iJAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgMDgVH9weGCkzXN03NjEaLePMMndtqeX7/aF044qb/5w7fktnCTz/L/2Lm/effV55c3QJevKm77++n8fjm27rv57WjJ827nlzS3Lbi5vDl21orxZ9O215Y1/J7Snl+62r2/u3a+XKACAgIgCAAiIKACAgIgCAAiIKACAgIgCAAiIKACAgIgCAAiIKACAgIgCAAiIKACAwJS/nQdMryVnP9jtI+yQP73yhvJmaGX9u2uRZrwzv6clT/xicX20rD7Zuvzl+giY5CUKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAj5ADNNgcN8l5c2Vi38y/QfpoDN+fGZ5s7ivQx8gnuHmb2q6fQRgCrxEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERBQAQMAHiGEa/H3k7eXNRAvn6KQlNz1b3sz0f+ZOefnkZ7t9BGAKvEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAoL9pmqbbhwAAmGm8RAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBgcKo/ODww0uY5Om5sYrTbR5jkbtsT3W1/f3ky//a3lTdL3/hUefPdw39a3rTF32173vvLC8ubOw65vrw5+KoV5c3iy9aWN710t/5u2zXX7tdLFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAASm/AFimCsG9xkqb27Y/2flzUVPHlHeMDUDhy3v9hF2yAX73VrePLZtS3kzdMvz5U1TXsDs5SUKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAj5ADNtpNj9X3qx8+rDyZvhNfytvesnTXzi6vFl46+P1X7RtvDw5+Ef3139PDzlh5xfLm4PWXFDeDP1xbXkDvMpLFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAREFABAQEQBAAR8gBi2M755c3nz5xOHypvfrtm/vDl+3/KkNXddvLq8WX9h/WPC/x5fUN4Mz99a3vSSQ9Z9urxZ8q315U1TXgCv5SUKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAiIKACAgogAAAv1N0/gGJQBAkZcoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACAxO9QeHB0baPEfHjU2MdvsIk9xte9xte9xte9xte9xtu+ba/XqJAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgICIAgAIiCgAgEB/0zRNtw8BADDTeIkCAAiIKACAgIgCAAiIKACAgIgCAAiIKACAgIgCAAiIKACAgIgCAAj8DwVsohrKnWRXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert to numpy\n",
    "patches_np = patches[0].numpy()\n",
    "\n",
    "# reshape (as pixels are flattened within patches)\n",
    "patches_np = np.reshape(patches_np, (-1, patch_size, patch_size))\n",
    "\n",
    "# and visualize\n",
    "for patch_idx, patch in enumerate(patches_np):\n",
    "    plt.subplot(image_height // patch_size, image_width // patch_size, patch_idx + 1)\n",
    "    plt.imshow(patch, vmin=0.0, vmax=1.0)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51e9b4",
   "metadata": {},
   "source": [
    "Now we can implement patch embedding, which is a simple linear projection of the patches. Here, we decided to implement it as a separate layer that wraps a `Dense` layer to make the model's final code more readable and give you a more straightforward setup for modifications. However, in this situation, it was completely unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68db5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.projection = tf.keras.layers.Dense(embedding_size)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        embedded_patches = self.projection(inputs)\n",
    "        return embedded_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884186f",
   "metadata": {},
   "source": [
    "Let's proceed further and embed our patches obtained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47dbe61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 49, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "embedded_patches = PatchEmbedding(embedding_size)(patches)\n",
    "embedded_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7fb75",
   "metadata": {},
   "source": [
    "Pay attention to how the tensor shapes changed so far:\n",
    "1. Input image [28, 28, 1]\n",
    "2. 4x4 patches [49, 16] = [(28/4)\\*(28/4), 4*4]\n",
    "3. Embedding (linear projection) [49, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f64bf",
   "metadata": {},
   "source": [
    "#### Class token\n",
    "\n",
    "Now we will add an 'artificial' **learnable** token to the embedding that will represent the output class. It has the same size as an embedded patch and is processed later in the same way as other patches.\n",
    "\n",
    "<img src=\"figures/vit_class_token.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1486599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AddClassToken, self).__init__()\n",
    "        \n",
    "    # note the use of the `build` method\n",
    "    # it is useful to define weights/sublayers that are dependent on the input shape\n",
    "    def build(self, input_shape):\n",
    "        self.class_token = self.add_weight(\n",
    "            name=\"class_token\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        patches_with_class_token = tf.map_fn(\n",
    "            lambda x: tf.concat((x, [self.class_token]), axis=0),\n",
    "            inputs\n",
    "        )\n",
    "        return patches_with_class_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612608e",
   "metadata": {},
   "source": [
    "As usual, let's apply this layer to the sample patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba7e44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_with_class_token = AddClassToken()(embedded_patches)\n",
    "patches_with_class_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00923af8",
   "metadata": {},
   "source": [
    "We can see now that there is a new 'patch' (50 vs 49), we can have a closer look at what is inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6efa28fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.07849021, -0.06710202, -0.00221198, -0.04001115,  0.03352876,\n",
       "       -0.00968865, -0.00481853,  0.04909171, -0.03814843, -0.02115183,\n",
       "       -0.05032863,  0.04577621, -0.04900284, -0.0309188 , -0.05680526,\n",
       "        0.00502711,  0.13155311, -0.05899636,  0.0195624 , -0.0625842 ,\n",
       "       -0.10022149, -0.00323665,  0.06680379,  0.01727597,  0.08662037,\n",
       "        0.09005643,  0.02771654,  0.03572049,  0.05492475, -0.0523171 ,\n",
       "        0.01757847, -0.07025711], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_with_class_token[0, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa82c3",
   "metadata": {},
   "source": [
    "As expected, something random. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62051bee",
   "metadata": {},
   "source": [
    "#### Positional embedding\n",
    "\n",
    "Transformers heavily rely on self-attention, which is a global operator and, at the same time, do not differentiate how spatially its inputs are related. Therefore, we have to implement something that encodes the location of the patches. In ViT, the authors propose to add **learnable** vectors to the embedded patches (and hope that they manage to learn the spatial dependencies, in the paper they managed).\n",
    "\n",
    "<img src=\"figures/vit_positional_encoding.png\" width=480 />\n",
    "\n",
    "Let's implement exactly the same idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e8a0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AddPositionalEmbedding, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            name=\"positional_embedding\",\n",
    "            shape=input_shape[1:],\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.positional_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abc84d",
   "metadata": {},
   "source": [
    "And we can check again if the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64296e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positionally_embedded_patches = AddPositionalEmbedding()(patches_with_class_token)\n",
    "positionally_embedded_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9d691",
   "metadata": {},
   "source": [
    "**However**, this naive approach is not appropriate when you are working with images of different sizes (and, thus, having different numbers of patches per image). One of the workarounds, in this case, is to interpolate positional embeddings, which is not implemented here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2143c",
   "metadata": {},
   "source": [
    "#### Transformer block\n",
    "\n",
    "Following the paper, we can easily implement the transformer block described there. Happily, `tf.keras` has all the required operations, we just need to combine them in the right order.\n",
    "\n",
    "<img src=\"figures/vit_encoder.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63a6eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, mlp_size, n_heads, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.mhsa = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=embedding_size // n_heads\n",
    "        )\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dense1 = tf.keras.layers.Dense(mlp_size, activation=tf.nn.gelu)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(embedding_size, activation=tf.nn.gelu)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # MHSA branch\n",
    "        x1 = self.norm1(inputs)\n",
    "        x1 = self.mhsa(x1, x1)\n",
    "        # residual connection 1\n",
    "        x2 = self.add([inputs, x1])\n",
    "        # MLP branch\n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.dense1(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        x3 = self.dense2(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        # residual connection 2\n",
    "        outputs = self.add([x2, x3])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73dc714",
   "metadata": {},
   "source": [
    "Run it to check on obvious bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58d326e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 50, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_block_output = TransformerBlock(32, 64, 4, 0.1)(positionally_embedded_patches)\n",
    "transformer_block_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841ad24",
   "metadata": {},
   "source": [
    "#### Extract class representation\n",
    "\n",
    "In ViT, the class of the whole image is represented by one token introduced in the very beginning (of course, there are alternatives, in the paper they consider global average pooling as well). We need to extract this token to classify further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ddb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractClassToken(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ExtractClassToken, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs[:, -1, :] # all images, last patch, all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedf79d",
   "metadata": {},
   "source": [
    "Traditionally, we are checking how our sample image is transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf60892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_token = ExtractClassToken()(transformer_block_output)\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53274822",
   "metadata": {},
   "source": [
    "#### Classification head\n",
    "\n",
    "As we are considering a classification problem, we would like to add a classification head to the model. A regular MLP is used in ViT, which is very straightforward.\n",
    "\n",
    "<img src=\"figures/vit_classifier.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e373833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, mlp_size, n_classes, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(mlp_size, activation=tf.keras.activations.tanh)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(n_classes, activation=tf.keras.activations.softmax)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden = self.dense1(inputs)\n",
    "        hidden = self.dropout(hidden)\n",
    "        outputs = self.dense2(hidden)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f11d3fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.11216382, 0.07497188, 0.02458979, 0.0657139 , 0.09987289,\n",
       "        0.08390631, 0.18306476, 0.08205833, 0.10313005, 0.17052825]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified = MLP(32, 10, 0.1)(class_token)\n",
    "classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f6aa1",
   "metadata": {},
   "source": [
    "#### Assembling everything together\n",
    "\n",
    "Fine, everything is prepared to build a ViT model.\n",
    "\n",
    "<img src=\"figures/vit.png\" width=480 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c70f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_classes,\n",
    "        patch_size=16, \n",
    "        embedding_size=768, \n",
    "        mlp_size=3072,\n",
    "        n_blocks=12, \n",
    "        n_heads=12, \n",
    "        dropout=0.1, \n",
    "        name=\"ViT\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ViT, self).__init__(name=name, **kwargs)\n",
    "        self.patch_extraction = PatchExtraction(patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(embedding_size)\n",
    "        self.add_class_token = AddClassToken()\n",
    "        self.add_positional_embedding = AddPositionalEmbedding()\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embedding_size, mlp_size, n_heads, dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ]\n",
    "        self.extract_class_token = ExtractClassToken()\n",
    "        self.mlp = MLP(mlp_size, n_classes, dropout)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        patches = self.patch_extraction(inputs)\n",
    "        patches = self.patch_embedding(patches)\n",
    "        patches = self.add_class_token(patches)\n",
    "        patches = self.add_positional_embedding(patches)\n",
    "        for block in self.transformer_blocks:\n",
    "            patches = block(patches)\n",
    "        class_token = self.extract_class_token(patches)\n",
    "        outputs = self.mlp(class_token)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd10f7",
   "metadata": {},
   "source": [
    "Now we can create a model and do some final checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6250420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ViT\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " patch_extraction_1 (PatchEx  multiple                 0         \n",
      " traction)                                                       \n",
      "                                                                 \n",
      " patch_embedding_2 (PatchEmb  multiple                 197376    \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " add_class_token_1 (AddClass  multiple                 768       \n",
      " Token)                                                          \n",
      "                                                                 \n",
      " add_positional_embedding_1   multiple                 787200    \n",
      " (AddPositionalEmbedding)                                        \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_3 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_4 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_5 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_6 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_8 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_9 (Transf  multiple                 7087872   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_10 (Trans  multiple                 7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_11 (Trans  multiple                 7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_12 (Trans  multiple                 7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_13 (Trans  multiple                 7087872   \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " extract_class_token_1 (Extr  multiple                 0         \n",
      " actClassToken)                                                  \n",
      "                                                                 \n",
      " mlp_1 (MLP)                 multiple                  5435368   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91,475,176\n",
      "Trainable params: 91,475,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_b16 = ViT(n_classes=1000)\n",
    "vit_b16.build(input_shape=(8, 512, 512, 1))\n",
    "vit_b16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fe58b",
   "metadata": {},
   "source": [
    "This model is ViT-B/16 as they call it in the paper. We can now compare the number of parameters that we got with the number of parameters reported by the authors. If they match, we are on the right track. So, excluding the classification head and working with 512x512 images, we have got $91\\,475\\,176 - 5\\,435\\,368 = 86\\,039\\,808$ parameters. \n",
    "\n",
    "The authors report 86M parameters, however, it is not very clear if it is for 512x512 images and whether they exclude the classification head. Anyway, the number is really close, therefore, we expect that our implementation is also correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "584f78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vit_b16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d9692",
   "metadata": {},
   "source": [
    "#### Training the model and evaluating it\n",
    "\n",
    "First, let's define another variant of ViT that is not too complicated for MNIST, we call it `vit-toy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8408ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ViT-Toy\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " patch_extraction_7 (PatchEx  multiple                 0         \n",
      " traction)                                                       \n",
      "                                                                 \n",
      " patch_embedding_8 (PatchEmb  multiple                 1088      \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " add_class_token_7 (AddClass  multiple                 64        \n",
      " Token)                                                          \n",
      "                                                                 \n",
      " add_positional_embedding_7   multiple                 3200      \n",
      " (AddPositionalEmbedding)                                        \n",
      "                                                                 \n",
      " transformer_block_50 (Trans  multiple                 33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_51 (Trans  multiple                 33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_52 (Trans  multiple                 33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_53 (Trans  multiple                 33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_54 (Trans  multiple                 33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_55 (Trans  multiple                 33472     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " extract_class_token_7 (Extr  multiple                 0         \n",
      " actClassToken)                                                  \n",
      "                                                                 \n",
      " mlp_7 (MLP)                 multiple                  9610      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 214,794\n",
      "Trainable params: 214,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit_toy = ViT(\n",
    "    n_classes=n_classes,\n",
    "    patch_size=4,\n",
    "    embedding_size=64,\n",
    "    mlp_size=128,\n",
    "    n_blocks=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    name=\"ViT-Toy\"\n",
    ")\n",
    "\n",
    "batch_size = 512\n",
    "vit_toy.build(input_shape=(batch_size, image_height, image_width, 1))\n",
    "vit_toy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404558fb",
   "metadata": {},
   "source": [
    "Compile it and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6b36391",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_toy.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    # note that the use of AdamW is a more common practice when training transformers\n",
    "    # however, we could not get good results with it for vit-toy and MNIST\n",
    "    #optimizer=tfa.optimizers.AdamW(learning_rate=1e-5, weight_decay=1e-4),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d9ac49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"vit_toy.h5\", monitor=\"val_loss\", save_best_only=True, save_weights_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\"vit_toy.csv\"),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=21, verbose=1, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.1, patience=10, verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad4b83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "94/94 [==============================] - 22s 183ms/step - loss: 1.3729 - categorical_accuracy: 0.4960 - val_loss: 0.5704 - val_categorical_accuracy: 0.8098 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.4920 - categorical_accuracy: 0.8376 - val_loss: 0.3311 - val_categorical_accuracy: 0.8962 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.3306 - categorical_accuracy: 0.8944 - val_loss: 0.2387 - val_categorical_accuracy: 0.9262 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.2446 - categorical_accuracy: 0.9227 - val_loss: 0.2406 - val_categorical_accuracy: 0.9257 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.2018 - categorical_accuracy: 0.9361 - val_loss: 0.1841 - val_categorical_accuracy: 0.9423 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.1820 - categorical_accuracy: 0.9425 - val_loss: 0.1683 - val_categorical_accuracy: 0.9463 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.1526 - categorical_accuracy: 0.9522 - val_loss: 0.1489 - val_categorical_accuracy: 0.9559 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "94/94 [==============================] - 16s 174ms/step - loss: 0.1342 - categorical_accuracy: 0.9577 - val_loss: 0.1257 - val_categorical_accuracy: 0.9606 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.1284 - categorical_accuracy: 0.9600 - val_loss: 0.1147 - val_categorical_accuracy: 0.9638 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.1102 - categorical_accuracy: 0.9651 - val_loss: 0.1173 - val_categorical_accuracy: 0.9641 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.1008 - categorical_accuracy: 0.9681 - val_loss: 0.1222 - val_categorical_accuracy: 0.9648 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0914 - categorical_accuracy: 0.9709 - val_loss: 0.1099 - val_categorical_accuracy: 0.9640 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0814 - categorical_accuracy: 0.9740 - val_loss: 0.1010 - val_categorical_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0784 - categorical_accuracy: 0.9750 - val_loss: 0.1101 - val_categorical_accuracy: 0.9668 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "94/94 [==============================] - 16s 174ms/step - loss: 0.0727 - categorical_accuracy: 0.9766 - val_loss: 0.0875 - val_categorical_accuracy: 0.9745 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0643 - categorical_accuracy: 0.9799 - val_loss: 0.0923 - val_categorical_accuracy: 0.9740 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0632 - categorical_accuracy: 0.9799 - val_loss: 0.0869 - val_categorical_accuracy: 0.9757 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0508 - categorical_accuracy: 0.9833 - val_loss: 0.0976 - val_categorical_accuracy: 0.9719 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0508 - categorical_accuracy: 0.9837 - val_loss: 0.0777 - val_categorical_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0516 - categorical_accuracy: 0.9830 - val_loss: 0.1077 - val_categorical_accuracy: 0.9693 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0472 - categorical_accuracy: 0.9842 - val_loss: 0.0816 - val_categorical_accuracy: 0.9753 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0440 - categorical_accuracy: 0.9854 - val_loss: 0.0856 - val_categorical_accuracy: 0.9751 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0407 - categorical_accuracy: 0.9865 - val_loss: 0.0812 - val_categorical_accuracy: 0.9765 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0391 - categorical_accuracy: 0.9867 - val_loss: 0.0957 - val_categorical_accuracy: 0.9752 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0330 - categorical_accuracy: 0.9892 - val_loss: 0.0778 - val_categorical_accuracy: 0.9797 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0345 - categorical_accuracy: 0.9884 - val_loss: 0.0987 - val_categorical_accuracy: 0.9743 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0375 - categorical_accuracy: 0.9877 - val_loss: 0.0869 - val_categorical_accuracy: 0.9772 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0314 - categorical_accuracy: 0.9896 - val_loss: 0.0817 - val_categorical_accuracy: 0.9777 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0332 - categorical_accuracy: 0.9890 - val_loss: 0.0771 - val_categorical_accuracy: 0.9791 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0293 - categorical_accuracy: 0.9904 - val_loss: 0.0928 - val_categorical_accuracy: 0.9759 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0265 - categorical_accuracy: 0.9911 - val_loss: 0.0960 - val_categorical_accuracy: 0.9765 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0235 - categorical_accuracy: 0.9925 - val_loss: 0.0795 - val_categorical_accuracy: 0.9803 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0250 - categorical_accuracy: 0.9915 - val_loss: 0.0895 - val_categorical_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0227 - categorical_accuracy: 0.9924 - val_loss: 0.0876 - val_categorical_accuracy: 0.9788 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0219 - categorical_accuracy: 0.9927 - val_loss: 0.0901 - val_categorical_accuracy: 0.9778 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0218 - categorical_accuracy: 0.9926 - val_loss: 0.0844 - val_categorical_accuracy: 0.9789 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0226 - categorical_accuracy: 0.9917 - val_loss: 0.0930 - val_categorical_accuracy: 0.9770 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0218 - categorical_accuracy: 0.9923 - val_loss: 0.0839 - val_categorical_accuracy: 0.9795 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0255 - categorical_accuracy: 0.9913\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0255 - categorical_accuracy: 0.9913 - val_loss: 0.1040 - val_categorical_accuracy: 0.9758 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0102 - categorical_accuracy: 0.9966 - val_loss: 0.0777 - val_categorical_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 41/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0065 - categorical_accuracy: 0.9980 - val_loss: 0.0776 - val_categorical_accuracy: 0.9818 - lr: 1.0000e-04\n",
      "Epoch 42/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0054 - categorical_accuracy: 0.9986 - val_loss: 0.0768 - val_categorical_accuracy: 0.9818 - lr: 1.0000e-04\n",
      "Epoch 43/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0045 - categorical_accuracy: 0.9989 - val_loss: 0.0765 - val_categorical_accuracy: 0.9820 - lr: 1.0000e-04\n",
      "Epoch 44/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0043 - categorical_accuracy: 0.9989 - val_loss: 0.0764 - val_categorical_accuracy: 0.9816 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0038 - categorical_accuracy: 0.9989 - val_loss: 0.0771 - val_categorical_accuracy: 0.9816 - lr: 1.0000e-04\n",
      "Epoch 46/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0039 - categorical_accuracy: 0.9989 - val_loss: 0.0767 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-04\n",
      "Epoch 47/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0031 - categorical_accuracy: 0.9993 - val_loss: 0.0754 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-04\n",
      "Epoch 48/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0031 - categorical_accuracy: 0.9993 - val_loss: 0.0757 - val_categorical_accuracy: 0.9827 - lr: 1.0000e-04\n",
      "Epoch 49/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0025 - categorical_accuracy: 0.9997 - val_loss: 0.0770 - val_categorical_accuracy: 0.9826 - lr: 1.0000e-04\n",
      "Epoch 50/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0026 - categorical_accuracy: 0.9994 - val_loss: 0.0782 - val_categorical_accuracy: 0.9817 - lr: 1.0000e-04\n",
      "Epoch 51/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0029 - categorical_accuracy: 0.9992 - val_loss: 0.0791 - val_categorical_accuracy: 0.9815 - lr: 1.0000e-04\n",
      "Epoch 52/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0024 - categorical_accuracy: 0.9995 - val_loss: 0.0776 - val_categorical_accuracy: 0.9818 - lr: 1.0000e-04\n",
      "Epoch 53/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0025 - categorical_accuracy: 0.9994 - val_loss: 0.0799 - val_categorical_accuracy: 0.9822 - lr: 1.0000e-04\n",
      "Epoch 54/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0022 - categorical_accuracy: 0.9995 - val_loss: 0.0787 - val_categorical_accuracy: 0.9825 - lr: 1.0000e-04\n",
      "Epoch 55/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0020 - categorical_accuracy: 0.9995 - val_loss: 0.0789 - val_categorical_accuracy: 0.9827 - lr: 1.0000e-04\n",
      "Epoch 56/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0018 - categorical_accuracy: 0.9998 - val_loss: 0.0795 - val_categorical_accuracy: 0.9833 - lr: 1.0000e-04\n",
      "Epoch 57/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0019 - categorical_accuracy: 0.9995\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0019 - categorical_accuracy: 0.9995 - val_loss: 0.0797 - val_categorical_accuracy: 0.9833 - lr: 1.0000e-04\n",
      "Epoch 58/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0017 - categorical_accuracy: 0.9996 - val_loss: 0.0793 - val_categorical_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Epoch 59/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0015 - categorical_accuracy: 0.9997 - val_loss: 0.0793 - val_categorical_accuracy: 0.9830 - lr: 1.0000e-05\n",
      "Epoch 60/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0015 - categorical_accuracy: 0.9997 - val_loss: 0.0791 - val_categorical_accuracy: 0.9829 - lr: 1.0000e-05\n",
      "Epoch 61/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0015 - categorical_accuracy: 0.9998 - val_loss: 0.0795 - val_categorical_accuracy: 0.9829 - lr: 1.0000e-05\n",
      "Epoch 62/2000\n",
      "94/94 [==============================] - 16s 173ms/step - loss: 0.0015 - categorical_accuracy: 0.9998 - val_loss: 0.0801 - val_categorical_accuracy: 0.9827 - lr: 1.0000e-05\n",
      "Epoch 63/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0015 - categorical_accuracy: 0.9997 - val_loss: 0.0800 - val_categorical_accuracy: 0.9826 - lr: 1.0000e-05\n",
      "Epoch 64/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0015 - categorical_accuracy: 0.9996 - val_loss: 0.0800 - val_categorical_accuracy: 0.9827 - lr: 1.0000e-05\n",
      "Epoch 65/2000\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0016 - categorical_accuracy: 0.9997 - val_loss: 0.0797 - val_categorical_accuracy: 0.9825 - lr: 1.0000e-05\n",
      "Epoch 66/2000\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0013 - categorical_accuracy: 0.9998 - val_loss: 0.0797 - val_categorical_accuracy: 0.9827 - lr: 1.0000e-05\n",
      "Epoch 67/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0015 - categorical_accuracy: 0.9998\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "94/94 [==============================] - 16s 172ms/step - loss: 0.0015 - categorical_accuracy: 0.9998 - val_loss: 0.0799 - val_categorical_accuracy: 0.9826 - lr: 1.0000e-05\n",
      "Epoch 68/2000\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.0014 - categorical_accuracy: 0.9998Restoring model weights from the end of the best epoch: 47.\n",
      "94/94 [==============================] - 16s 171ms/step - loss: 0.0014 - categorical_accuracy: 0.9998 - val_loss: 0.0799 - val_categorical_accuracy: 0.9827 - lr: 1.0000e-06\n",
      "Epoch 68: early stopping\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 2000\n",
    "history = vit_toy.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=training_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2a815",
   "metadata": {},
   "source": [
    "And evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b56cd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.0655 - categorical_accuracy: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06549768894910812, 0.9847000241279602]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_toy.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a12216",
   "metadata": {},
   "source": [
    "Not the state-of-the-art but still impressive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
